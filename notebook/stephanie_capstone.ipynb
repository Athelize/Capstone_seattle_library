{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5543eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import dask.dataframe as dd\n",
    "import re\n",
    "from dask.diagnostics import ProgressBar\n",
    "import dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66f412f",
   "metadata": {},
   "source": [
    "**Step One**\n",
    "After importing the packages follow these steps to filter the initial dataset from: \n",
    "https://data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6/data\n",
    "\n",
    "To just the years 2018 - Present.\n",
    "\n",
    "**Note**: Dask is used throughout this Notebook instead of Pandas due to the size of the dataset and the limitations of my computer. If your computer is able to handle large data calculations you do not need to repeatedly save different versions of the data. You can simply perform the cleaning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba5bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seattle_df = dd.read_csv('..\\data\\checkouts_by_title.csv', dtype={'ISBN': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41928a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_seattle_df = seattle_df[seattle_df['CheckoutYear'] >= 2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b91cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_seattle_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd87137",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = filtered_seattle_df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa50efb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result.to_csv('..\\\\data\\\\filtered_checkouts_by_title.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869f1c5e",
   "metadata": {},
   "source": [
    "**Step Two** After performing step one you can load the newly filtered data and review the data following the steps below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3673f9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seattle_df = dd.read_csv('..\\data\\\\filtered_checkouts_by_title.csv', dtype={'ISBN': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec9704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seattle_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23ae9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seattle_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7daf2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = seattle_df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0179eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39713880",
   "metadata": {},
   "source": [
    "**Step Three** Cleaning the ISBN column by splitting it into the ISBN-10 and the ISBN-13 column as the current data contains both ISBN numbers as comma separated values in the same column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf74dc4b",
   "metadata": {},
   "source": [
    "Finding the corresponding lengths to sort into the correct columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0caca6",
   "metadata": {},
   "source": [
    "Trying for loop and chunk iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e24e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(chunk):\n",
    "    chunk_copy = chunk.copy()\n",
    "    chunk_copy['ISBN'] = chunk_copy['ISBN'].astype(str)\n",
    "    split_isbn = chunk_copy['ISBN'].str.split(',', expand=True)\n",
    "    chunk_copy.loc[:, 'Possible ISBN-10'] = split_isbn[0].str.strip()\n",
    "    chunk_copy.loc[:, 'Possible ISBN-13'] = None\n",
    "    if split_isbn.shape[1] > 1:\n",
    "        chunk_copy.loc[:, 'Possible ISBN-13'] = split_isbn[1].str.strip()\n",
    "    chunk_copy.loc[:, 'ISBN-10'] = chunk_copy['Possible ISBN-10'].where(chunk_copy['Possible ISBN-10'].str.len() == 10, np.nan)\n",
    "    chunk_copy.loc[:, 'ISBN-13'] = chunk_copy['Possible ISBN-13'].where(chunk_copy['Possible ISBN-13'].str.len() == 13, chunk_copy['Possible ISBN-10'])\n",
    "    chunk_copy = chunk_copy.drop(columns=['Possible ISBN-10', 'Possible ISBN-13'])\n",
    "    return chunk_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd33e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 10**5\n",
    "chunks = [process_chunk(result[i:i+chunk_size]) for i in range(0, result.shape[0], chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3749fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_processed = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830d9530",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da00673",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41530495",
   "metadata": {},
   "source": [
    "**Optional** This step is just to check if one or the other has complete data which they do not. Both have null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5706e0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_processed['ISBN-13'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95923083",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_processed['ISBN-10'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c33697",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_processed['PublicationYear'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd79329",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_processed.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51b41ea",
   "metadata": {},
   "source": [
    "**Step Four** The following steps are for cleaning the publication year column. You can skip to step **4.5** if you want to use the cumulative function that performs all of the steps below at once.\n",
    "\n",
    "**Function One** Leaves null values as null values. Converts year to string. Keeps only digit values that are in year for cases like [1990] and finally checks if the length of the result is 4 digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6149d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_year(year):\n",
    "    if pd.isna(year):\n",
    "        return year \n",
    "    year = str(year) \n",
    "    cleaned_year = ''.join(filter(str.isdigit, year))\n",
    "    return cleaned_year[:4] if len(cleaned_year) >= 4 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438436e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_processed['Cleaned_Year'] = result_processed['PublicationYear'].apply(clean_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422979d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_processed['Cleaned_Year'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189d4051",
   "metadata": {},
   "source": [
    "**Function Two** Leaves null values as nan values. Converts year to string. Similar to function one but attempts to clean better by using a lamda function instead to capture the years better. Then it taked the last four digits and uses that as the year and checks if the result is 4 characters otherwise it will leave it as nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f26e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_year(year):\n",
    "    if pd.isna(year):\n",
    "        return np.nan\n",
    "    year = str(year)\n",
    "    year = ''.join(filter(lambda x: x.isdigit(), year))\n",
    "    year = year[-4:]\n",
    "    return year if len(year) == 4 else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e41bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_processed['Cleaned_Year'] = result_processed['PublicationYear'].apply(clean_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db9fc6",
   "metadata": {},
   "source": [
    "**Optional** Checking if all the years are clean or if there are still outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4279818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_processed['Cleaned_Year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b80a2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_processed[result_processed['Cleaned_Year'] == '92009']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db64438",
   "metadata": {},
   "source": [
    "**Function Three** Checks if the year is a string. Uses regex to find years in the 1900-1999 format. If so it takes the first four digit year group. then returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1effa98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_year(year):\n",
    "    if isinstance(year, str):\n",
    "        match = re.search(r\"\\[(\\d{4})-(\\d{2,4})\\]\", year)\n",
    "        if match:\n",
    "            return match.group(1)  \n",
    "        \n",
    "        match = re.match(r\"(\\d{4})(\\d{4})\", year)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        \n",
    "    return year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee604d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_processed['Cleaned_Year'] = result_processed['PublicationYear'].apply(clean_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bc9406",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_processed['Cleaned_Year'] = pd.to_numeric(result_processed['Cleaned_Year'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae97cc7",
   "metadata": {},
   "source": [
    "**Step 4.2** First half of manual cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3f7573",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_processed['Cleaned_Year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5087c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_processed[result_processed['Cleaned_Year'] == 2022.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9293d60",
   "metadata": {},
   "source": [
    "**Step 4.2** Second half of manual cleaning of outliers. Look at specific rows to fix by identifying outliers in the steps from the first half then putting the unique index in the code below and updating it accordingly. Ex. 2022.1 -> Index 12303238 -> 2022. After that replace it using the .loc lines for both the 'PublicationYear' and the 'Cleaned_Year' columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1513633",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_update = [12303238]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae62cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_year = 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baee49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_processed.loc[indexes_to_update, 'PublicationYear'] = new_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd77e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_processed.loc[indexes_to_update, 'Cleaned_Year'] = new_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4043bbef",
   "metadata": {},
   "source": [
    "**Step 4.5** Instead of running the three functions from step 4 individually you can run this function which combines their actions into one larger function to achieve the same effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd50e725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_year(year):\n",
    "    if pd.isna(year):\n",
    "        return np.nan\n",
    "    \n",
    "    year = str(year)\n",
    "    match = re.search(r\"\\[(\\d{4})-(\\d{2,4})\\]\", year)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    match = re.match(r\"(\\d{4})(\\d{4})\", year)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    year = ''.join(filter(lambda x: x.isdigit(), year))\n",
    "    return year[-4:] if len(year) == 4 else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f387b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_processed['Cleaned_Year'] = result_processed['PublicationYear'].apply(clean_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ad82fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_processed['Cleaned_Year'] = pd.to_numeric(result_processed['Cleaned_Year'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6abeafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_processed['Cleaned_Year'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28383d8a",
   "metadata": {},
   "source": [
    "**Optional** Change the 'Cleaned_Year' column back into Int64 data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03db443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_processed['Cleaned_Year'] = result_processed['Cleaned_Year'].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62f8a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_processed.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2d0215",
   "metadata": {},
   "source": [
    "**Optional** Save this data to a new CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe1471",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_processed.to_csv('cleaned_checkouts_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688a425e",
   "metadata": {},
   "source": [
    "**Step Five** Load the new file or move on to the next portion of data cleaning if you skipped the multiple csv file saves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd048e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'ISBN': 'object',\n",
    "    'ISBN-10': 'object',\n",
    "    'ISBN-13': 'object'\n",
    "}\n",
    "seattle_df = dd.read_csv('..\\data\\cleaned_checkouts_data.csv', dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8123ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = seattle_df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f9e876",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b59e94",
   "metadata": {},
   "source": [
    "**Optional** Look at the unique subjects as the next cleaning step will be to attempt to clean the 'Subjects' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcc0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['Subjects'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2847723",
   "metadata": {},
   "source": [
    "**Optional** The following two pieces of code are to toggle the display limits so that you can see any number of rows you like when running the value counts code. \n",
    "\n",
    "**Note** If you do not toggle it back off you run a risk of crashing the kernel if you try to look at too many rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3a842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840eff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f7ad6e",
   "metadata": {},
   "source": [
    "**Optional** Useful to get an idea of what the subjects look like and what still needs to be cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73cd028",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['Subjects'].value_counts().head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69681337",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['Subjects'].value_counts().tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c4831e",
   "metadata": {},
   "source": [
    "**Step Six** Function to clean based on CSV value separations on the 'Subject' column. It will take the first two values in a comma separated list and put them into a pair that can then be split into new columns.\n",
    "\n",
    "**Note** Any time you need to rerun this you need to start from the data loading step or it will not run correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfff13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_subject(subject):\n",
    "    if isinstance(subject, str):\n",
    "        keywords = [word.strip() for word in subject.split(',')]\n",
    "        primary_category = keywords[0] if keywords else 'Uncategorized'\n",
    "        secondary_category = keywords[1] if len(keywords) > 1 else 'General'\n",
    "        return primary_category, secondary_category\n",
    "    return 'Uncategorized', 'Uncategorized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3c31ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.from_pandas(result, npartitions=8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81018712",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf['Cleaned_Categories'] = ddf['Subjects'].apply(clean_subject, meta=('x', 'f8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d29891e",
   "metadata": {},
   "source": [
    "**Optional** Troubleshooting step in case there are issues with the function from step 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e3f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ddf['Cleaned_Categories'].compute().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89705d65",
   "metadata": {},
   "source": [
    "**Step Six** Function that extracts the categories from the previous steps, verifies the length, and if anything fails will return \"none\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bd07fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_category(categories, position):\n",
    "    if categories and isinstance(categories, tuple) and len(categories) > position:\n",
    "        return categories[position]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caede4d",
   "metadata": {},
   "source": [
    "**Step Seven** Code that assigns the subjects to new columns. The first value in the comma separated list is assigned to the 'Cleaned_Primary' column and the second value is assigned to the \"Cleaned_Secondary' column.\n",
    "\n",
    "**Note** Dask will not run if you try to combine these into one line of code. Each column has to be handled one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577256e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf['Cleaned_Primary'] = ddf['Cleaned_Categories'].apply(lambda x: extract_category(x, 0), meta=('x', 'O'))\n",
    "ddf['Cleaned_Secondary'] = ddf['Cleaned_Categories'].apply(lambda x: extract_category(x, 1), meta=('x', 'O'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e086fa7",
   "metadata": {},
   "source": [
    "**Step Eight** Dropping the 'Cleaned_Categories' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffddf62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.drop(columns=['Cleaned_Categories'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c483f02",
   "metadata": {},
   "source": [
    "**Step Nine** Getting the cleaned result and verifying the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d6b693",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    result_dask = ddf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb371b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dask.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e50a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dask.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b4a2ff",
   "metadata": {},
   "source": [
    "**Step Ten** Function to sort the data into periods. Pre-pandemic 2018-2019. Pandemic 2020-2021, and Post-Pandemic 2022-2023. \n",
    "\n",
    "**Note** Years can be changed if you want to look at different years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1ea7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_period(year):\n",
    "    if 2018 <= year <= 2019:\n",
    "        return 'Pre_Pandemic'\n",
    "    elif 2020 <= year <= 2021: \n",
    "        return 'Pandemic'\n",
    "    elif 2022 <= year <= 2023:\n",
    "        return 'Post_Pandemic'\n",
    "    else:\n",
    "        return 'Undefined'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84f8822",
   "metadata": {},
   "source": [
    "**Optional** Converting data back to dask to handle the assignment of periods. \n",
    "\n",
    "**Note** Can also be done with Pandas if your computer can handle large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae41b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.from_pandas(result_dask, npartitions=8) if isinstance(result_dask, pd.DataFrame) else result_dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac480e",
   "metadata": {},
   "source": [
    "**Step Eleven** Adding the new 'Period' column using the function and the dask dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b82cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf['Period'] = ddf['CheckoutYear'].apply(assign_period, meta=('x', 'O'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22e6fd2",
   "metadata": {},
   "source": [
    "**Step Twelve** Getting the results and converting back to a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135cafd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    result_dask_updated = ddf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7d9694",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dask_updated.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08fcf7b",
   "metadata": {},
   "source": [
    "**Optional** Another conversion of the 'Cleaned_Year' column to Int64 datatype if you missed doing it earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57133426",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dask_updated['Cleaned_Year'] = result_dask_updated['Cleaned_Year'].astype('Int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3347d27d",
   "metadata": {},
   "source": [
    "**Step Thirteen** Saving the final cleaned file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91788a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dask_updated.to_csv('cleanedv2_checkouts_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959603c6",
   "metadata": {},
   "source": [
    "**End Comments** This file can now be loaded into Tableau for dashboarding purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc343e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
